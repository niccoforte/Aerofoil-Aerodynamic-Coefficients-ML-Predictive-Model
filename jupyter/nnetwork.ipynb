{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb434d3b-9eae-4050-a448-f4e7942655c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run cases.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e757093-27c9-47c5-b2fd-f02dd3bfe6ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential, optimizers\n",
    "from keras.layers import Dense, BatchNormalization, Normalization, Activation, InputLayer, LeakyReLU, PReLU, Dropout\n",
    "from keras.metrics import mean_squared_error, mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cc026a-f9f6-425a-b028-37ceb04aa0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(df1, df2=None, nTrain=100, nTest=50, p_foil=None, p_Re=None):\n",
    "    NNdata_df = df1.drop(columns = ['spline', 'xy_profile'])\n",
    "               \n",
    "    if df2 is not None:\n",
    "        NNexp_df = df2.drop(columns = ['spline', 'xy_profile'])\n",
    "\n",
    "        if p_foil:\n",
    "            test_df = NNexp_df[NNexp_df.file == p_foil]\n",
    "        else:\n",
    "            test_df = NNexp_df\n",
    "        \n",
    "        if p_Re:\n",
    "            test_df = test_df[test_df.Re == p_Re]\n",
    "\n",
    "        t_indx = [random.randint(0, len(set(NNdata_df.file))-1) for i in range(nTrain)]\n",
    "        t_foils = [list(set(NNdata_df.file))[indx] for indx in t_indx]\n",
    "        if p_foil:\n",
    "            t_foils = [i for i in t_foils if aerofoil_difference(df=df1, name1=i, name2=p_foil) > 0.5]\n",
    "\n",
    "        train_df = shuffle(NNdata_df[NNdata_df.file.str.contains(\"|\".join(t_foils))])\n",
    "        if p_foil:\n",
    "            train_df = train_df[train_df.file != p_foil]\n",
    "        if p_Re:\n",
    "            train_df = train_df[train_df.Re != p_Re]\n",
    "\n",
    "        train_df['weights'] = [1]*len(train_df)\n",
    "\n",
    "        if p_foil:\n",
    "            Texp_df = NNexp_df[NNexp_df.file != p_foil]\n",
    "        else:\n",
    "            Texp_df = NNexp_df\n",
    "        \n",
    "        if p_Re:\n",
    "            Texp_df = Texp_df[Texp_df.Re != p_Re]\n",
    "\n",
    "        Texp_df['weights'] = [5]*len(Texp_df)\n",
    "        Texp_df.loc[Texp_df.alpha > 10, 'weights'] = 10\n",
    "        Texp_df.loc[Texp_df.alpha < -10, 'weights'] = 10\n",
    "\n",
    "        train_df = train_df[~train_df.file.str.contains(\"|\".join(list(set(Texp_df.file))))]\n",
    "\n",
    "        train_df = shuffle(pd.concat([train_df, Texp_df]))\n",
    "        \n",
    "        print(f' N. Xfoil Training Aerofoils: {len(set(train_df.file.tolist()))}')\n",
    "        print(f' N. Xfoil in Training: {len(train_df)}')\n",
    "        print(f' N. Exp. Training Aerofoils: {len(set(Texp_df.file.tolist()))}')\n",
    "        print(f' N. Experimental in Training: {len(Texp_df)}')\n",
    "        \n",
    "    else:\n",
    "        if p_foil:\n",
    "            test_df = NNdata_df[NNdata_df.file == p_foil]\n",
    "            if p_Re:\n",
    "                test_df = test_df[test_df.Re == p_Re]\n",
    "\n",
    "            t_indx = [random.randint(0, len(set(NNdata_df.file))-1) for i in range(nTrain)]\n",
    "            t_foils = [list(set(NNdata_df.file))[indx] for indx in t_indx]\n",
    "            t_foils = [i for i in t_foils if aerofoil_difference(df=df1, name1=i, name2=p_foil) > 0.5]\n",
    "\n",
    "            train_df = shuffle(NNdata_df[NNdata_df.file.str.contains(\"|\".join(t_foils))])\n",
    "            train_df = train_df[train_df.file != p_foil]\n",
    "            if p_Re:\n",
    "                train_df = train_df[train_df.Re != p_Re]\n",
    "            \n",
    "            print(f' N. Xfoil Training Aerofoils: {len(t_foils)}')\n",
    "            print(f' N. Xfoil in Training: {len(train_df)}')\n",
    "        \n",
    "        else:\n",
    "            p_indx = [random.randint(0, len(set(NNdata_df.file))-1) for i in range(nTest)]\n",
    "            p_foils = [list(set(NNdata_df.file))[indx] for indx in p_indx]\n",
    "\n",
    "            test_df = NNdata_df[NNdata_df['file'].str.contains(f\"|\".join(p_foils))]\n",
    "            if p_Re:\n",
    "                test_df = test_df[test_df.Re == p_Re]\n",
    "\n",
    "            t_indx = [random.randint(0, len(set(df1.file))-1) for i in range(nTrain)]\n",
    "            t_foils = [list(set(df1.file))[indx] for indx in t_indx if indx not in p_indx]\n",
    "\n",
    "            train_df = shuffle(NNdata_df[NNdata_df.file.str.contains(\"|\".join(t_foils))])\n",
    "            if p_Re:\n",
    "                train_df = train_df[train_df.Re != p_Re]\n",
    "            \n",
    "            print(f' N. Xfoil Training Aerofoils: {len(t_foils)}')\n",
    "            print(f' N. Xfoil in Training: {len(train_df)}')\n",
    "            \n",
    "        train_df['weights'] = [1]*len(train_df)\n",
    "        train_df.loc[train_df.alpha > 10, 'weights'] = 2\n",
    "        train_df.loc[train_df.alpha < -10, 'weights'] = 2\n",
    "    \n",
    "    sample_weights = np.array(train_df.weights.tolist())\n",
    "\n",
    "    return train_df, test_df, sample_weights\n",
    "\n",
    "\n",
    "def prep_data(data):\n",
    "    train_in = np.array([[0.0 if math.isnan(y) else y for y in ys_up] +\n",
    "                         [0.0 if math.isnan(y) else y for y in ys_low] +\n",
    "                         [float(Re)] + [float(alpha)] for ys_up, ys_low, Re, alpha in \n",
    "                         zip(data[0].y_up.tolist(), data[0].y_low.tolist(), data[0].Re.tolist(), data[0].alpha.tolist())], \n",
    "                        dtype='float32')\n",
    "\n",
    "    train_out = np.array([[float(cl), float(cd)] for cl, cd in zip(data[0].Cl.tolist(), data[0].Cd.tolist())], \n",
    "                         dtype='float32')\n",
    "\n",
    "    test_in = np.array([[0.0 if math.isnan(y) else y for y in ys_up] +\n",
    "                        [0.0 if math.isnan(y) else y for y in ys_low] +\n",
    "                        [float(Re)] + [float(alpha)] for ys_up, ys_low, Re, alpha in \n",
    "                        zip(data[1].y_up.tolist(), data[1].y_low.tolist(), data[1].Re.tolist(), data[1].alpha.tolist())], \n",
    "                       dtype='float32')\n",
    "\n",
    "    test_out = np.array([[float(cl), float(cd)] for cl, cd in zip(data[1].Cl.tolist(), data[1].Cd.tolist())], \n",
    "                        dtype='float32')\n",
    "\n",
    "    return train_in, train_out, test_in, test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91895420-164b-4613-a9e3-3db432816b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    indx = 0\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        if np.abs(t - p) < 0.01:\n",
    "            indx+=1\n",
    "        else:\n",
    "            pass\n",
    "    score = indx / len(y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a9651-a4af-4bd2-afd5-e75601aeecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, sample_weights = get_data(df1=data_df, df2=exp_data_df, nTrain=250, nTest=50, p_foil='n0012', p_Re=1000000)\n",
    "train_in, train_out, test_in, test_out = prep_data(data=[train_df, test_df])\n",
    "dat = [train_in, train_out, test_in, test_out]\n",
    "\n",
    "print(len(train_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b1843-854e-40f5-ba0f-9fd91f912ab6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ren_train_df, ren_test_df = get_data(df1=ren_data_df, df2=exp_data_df, nTrain=250, nTest=50)#, p_foil='n0012', p_Re=1000000)\n",
    "# ren_train_in, ren_train_out, ren_test_in, ren_test_out = prep_data(data=[ren_train_df, ren_test_df])\n",
    "# ren_dat = [ren_train_in, ren_train_out, ren_test_in, ren_test_out]\n",
    "\n",
    "# print(len(ren_train_df), len(ren_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61bed4-70a0-40ad-b044-6ec005da7e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, data, neurons, activation, weights, name, test_df, EPOCHS=50, BATCH=256, lr=0.001, verbose=0, callbacks=None):\n",
    "        self.train_in = data[0]\n",
    "        self.train_out = data[1]\n",
    "        self.test_in = data[2]\n",
    "        self.test_out = data[3]\n",
    "        self.neurons = neurons\n",
    "        self.activation = activation\n",
    "        self.weights = weights\n",
    "        self.name = name\n",
    "        self.test_df = test_df\n",
    "        self.lr = lr\n",
    "        self.EPOCHS = EPOCHS\n",
    "        self.BATCH = BATCH\n",
    "        self.verbose = verbose\n",
    "        self.callbacks = callbacks\n",
    "        \n",
    "        self.model = None\n",
    "        self.fitHistory = None\n",
    "        self.trainEv = None\n",
    "        self.testEv = None\n",
    "        self.pred = None\n",
    "        self.Pmetrics_df = None\n",
    "        self.output_df = None\n",
    "        \n",
    "        self.model = self.build_MLP()\n",
    "        self.fitHistory = self.train()\n",
    "        self.trainEv, self.testEv = self.evaluate()\n",
    "        self.pred, self.Pmetrics_df, self.output_df = self.predict()\n",
    "        \n",
    "    \n",
    "    def build_MLP(self):\n",
    "        model = Sequential(name=self.name)\n",
    "        \n",
    "        model.add(InputLayer(input_shape=len(self.train_in[0])))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        for n in self.neurons[1]:\n",
    "            model.add(Dense(self.neurons[0][n]))\n",
    "\n",
    "            if self.activation == 'leakyrelu':\n",
    "                model.add(LeakyReLU(alpha=0.3))\n",
    "            elif self.activation == 'prelu':\n",
    "                model.add(PReLU(alpha_initializer='zeros'))\n",
    "            else:\n",
    "                model.add(Activation(self.activation))\n",
    "            \n",
    "            model.add(BatchNormalization())\n",
    "            #model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(Dense(len(self.test_out[0])))\n",
    "\n",
    "        OPT = optimizers.Adam(learning_rate=self.lr, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "        METS = ['ACC', 'MAE', 'MSE']\n",
    "        model.compile(optimizer=OPT, loss='MSE', metrics=METS, weighted_metrics=METS)#, loss_weights=[1,2])\n",
    "\n",
    "        if self.verbose == 1:\n",
    "            print(model.summary())\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=self.verbose, mode='min')\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=5, verbose=self.verbose, \n",
    "                                      min_delta=1e-4, mode='min')\n",
    "        if self.callbacks:\n",
    "            self.callbacks = [reduce_lr] #, early_stop]\n",
    "\n",
    "        fitHistory = self.model.fit(self.train_in, self.train_out, epochs=self.EPOCHS, batch_size=self.BATCH, \n",
    "                                    validation_split=0.1, verbose=self.verbose, callbacks=self.callbacks, \n",
    "                                    sample_weight=self.weights) #, class_weight={0:1, 1:1.5})\n",
    "        \n",
    "        return fitHistory\n",
    "    \n",
    "    \n",
    "    def evaluate(self):\n",
    "        trainEv = self.model.evaluate(self.train_in, self.train_out, batch_size=self.BATCH)\n",
    "        testEv = self.model.evaluate(self.test_in, self.test_out, batch_size=self.BATCH)\n",
    "\n",
    "        return trainEv, testEv\n",
    "    \n",
    "    \n",
    "    def predict(self, test_in=None, test_out=None, test_df=None):\n",
    "        if test_in is None:\n",
    "            test_in = self.test_in\n",
    "            test_out = self.test_out\n",
    "            test_df = self.test_df\n",
    "        \n",
    "        pred = self.model.predict(test_in)\n",
    "    \n",
    "        clp = [p[0] for p in pred]\n",
    "        cdp = [p[1] for p in pred]\n",
    "        ldp = [l/d for l,d in zip(clp,cdp)]\n",
    "        clt = [t[0] for t in test_out]\n",
    "        cdt = [t[1] for t in test_out]\n",
    "        ldt = [l/d for l,d in zip(clt,cdt)]\n",
    "        p = clp + cdp\n",
    "        t = clt + cdt\n",
    "\n",
    "        ACC_cl = accuracy(clt, clp)\n",
    "        ACC_cd = accuracy(cdt, cdp)\n",
    "        ACC = accuracy(t, p)\n",
    "        R2_cl = r2_score(clt, clp)\n",
    "        R2_cd = r2_score(cdt, cdp)\n",
    "        MSE_cl = mean_squared_error(clt, clp)\n",
    "        MSE_cd = mean_squared_error(cdt, cdp)\n",
    "        MSE = mean_squared_error(t, p)\n",
    "        MAE_cl = mean_absolute_error(clt, clp)\n",
    "        MAE_cd = mean_absolute_error(cdt, cdp)\n",
    "        MAE = mean_absolute_error(t, p)\n",
    "        RMSE_cl = math.sqrt(MSE_cl)\n",
    "        RMSE_cd = math.sqrt(MSE_cd)\n",
    "        RMSE = math.sqrt(MSE)\n",
    "        Pmetrics_df = pd.DataFrame({'name': [str(self.name)], \n",
    "                                    'ACC_cl': [float(ACC_cl)], 'ACC_cd': [float(ACC_cd)], 'ACC': [float(ACC)],\n",
    "                                    'MAE_cl': [float(MAE_cl)], 'MAE_cd': [float(MAE_cd)], 'MAE': [float(MAE)], \n",
    "                                    'R2_cl': [float(R2_cl)], 'R2_cd': [float(R2_cd)], \n",
    "                                    'MSE_cl': [float(MSE_cl)], 'MSE_cd': [float(MSE_cd)], 'MSE': [float(MSE)], \n",
    "                                    'RMSE_cl': [float(RMSE_cl)], 'RMSE_cd': [float(RMSE_cd)], 'RMSE': [float(RMSE)]})\n",
    "        \n",
    "        output_df = test_df.copy()\n",
    "        output_df = output_df.drop(columns=['x', 'y_up', 'y_low'])\n",
    "        output_df['LtD'] = ldt\n",
    "        output_df['Cl_pred'] = clp\n",
    "        output_df['Cd_pred'] = cdp\n",
    "        output_df['LtD_pred'] = ldp\n",
    "\n",
    "        return pred, Pmetrics_df, output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f938aa-c97e-407a-b249-376b062e76b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_Model(data, neurons, activation, weights, test_df, EPOCHS=50, BATCH=256, lr=0.1, verbose=0, callbacks=None):\n",
    "    models = {}\n",
    "    for act in activation:\n",
    "        name = f'MLP-{act.capitalize()}'\n",
    "        print(f'  ====  {name}  ====')\n",
    "        model = Model(data=data, \n",
    "                      neurons=neurons, \n",
    "                      activation=act, \n",
    "                      weights=weights,\n",
    "                      name=name, \n",
    "                      test_df=test_df,\n",
    "                      EPOCHS=EPOCHS, \n",
    "                      BATCH=BATCH, \n",
    "                      lr=lr,\n",
    "                      verbose=verbose,\n",
    "                      callbacks=callbacks)\n",
    "\n",
    "        models[name] = model\n",
    "    return models\n",
    "    \n",
    "\n",
    "def model_predict(model, test_in, test_out, test_df):\n",
    "    pred, Pmetrics_df, output_df = model.predict(test_in, test_out, test_df)\n",
    "    \n",
    "    return pred, Pmetrics_df, output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad6509-3091-4c2e-8117-2fee0309feb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "neurons = [[512, 256, 128, 64, 32, 16], [1,2,3,4]]\n",
    "activations = ['sigmoid'] #, 'hard_sigmoid', 'softmax', 'softsign', 'leakyrelu', 'selu', 'elu', 'prelu', 'tanh']\n",
    "activationz = ['swish', 'softplus', 'relu', 'gelu']\n",
    "\n",
    "models = run_Model(data=dat, \n",
    "                   neurons=neurons, \n",
    "                   activation=activations, \n",
    "                   weights=sample_weights,\n",
    "                   test_df=test_df, \n",
    "                   EPOCHS=100, \n",
    "                   BATCH=256, \n",
    "                   lr=0.001, \n",
    "                   verbose=0,\n",
    "                   callbacks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce4c87f-ca12-4f60-8bf1-dbf3d21c846f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = list(models.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab6fe1-68c5-4ce3-b9a6-6e043aa430aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, Pmetrics_df, output_df = model.pred, model.Pmetrics_df, model.output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f0892-bb42-4e1f-aa97-30516418db3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred, Pmetrics_df, output_df = model_predict(model=model, test_in=test_in, test_out=test_out, test_df=test_df)\n",
    "#pred, Pmetrics_df, output_df = model_predict(model=model, test_in=ren_test_in, test_out=ren_test_out, test_df=ren_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e51ca-6012-41e1-ae65-fc18229215ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred_metrics(Pmetrics_df, models, file='results/metrics/prediction-mets.csv', df_from='current', add=False, \n",
    "                 df_save=False, prnt=False, plot=False):\n",
    "    if df_from == 'current':\n",
    "        metrics_df = Pmetrics_df\n",
    "        metrics_df['DateTime'] = [str(datetime.now())] * len(metrics_df)\n",
    "    \n",
    "    elif df_from == 'models':\n",
    "        metrics_df = pd.DataFrame()\n",
    "        for name, model in models.items():\n",
    "            new_row = model.Pmetrics_df\n",
    "            metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n",
    "        metrics_df['DateTime'] = [str(datetime.now())] * len(metrics_df)\n",
    "            \n",
    "    elif df_from == 'file':\n",
    "        metrics_df = pd.read_csv(file, index_col=0)\n",
    "        \n",
    "        if add == 'current':\n",
    "            new_row = Pmetrics_df\n",
    "            new_row['DateTime'] = [str(datetime.now())] * len(new_row)\n",
    "            metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n",
    "\n",
    "        elif add == 'models':\n",
    "            for name, model in models.items():\n",
    "                new_row = model.Pmetrics_df\n",
    "                new_row['DateTime'] = [str(datetime.now())] * len(new_row)\n",
    "                metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)\n",
    "    \n",
    "    if df_save:\n",
    "        metrics_df.to_csv(file)\n",
    "        \n",
    "    if prnt:\n",
    "        print('=== PREDICTION METRICS ===')\n",
    "        for index, row in metrics_df.iterrows():\n",
    "            print(f' MODEL: {row[0]}.')\n",
    "            print(f'   ACC:  CL: {round(row[1], 6)} and CD: {round(row[2], 6)} and ALL: {round(row[3], 6)}.')\n",
    "            print(f'   MAE:  CL: {round(row[4], 6)} and CD: {round(row[5], 6)} and ALL: {round(row[6], 6)}.')\n",
    "            print(f'    R2:  CL: {round(row[7], 6)} and CD: {round(row[8], 6)}.')\n",
    "            print(f'   MSE:  CL: {round(row[9], 6)} and CD: {round(row[10], 6)} and ALL: {round(row[11], 6)}.')\n",
    "            print(f'  RMSE:  CL: {round(row[12], 6)} and CD: {round(row[13], 6)} and ALL: {round(row[14], 6)}.', '\\n')\n",
    "        \n",
    "    if plot:\n",
    "        names = list(metrics_df.name)\n",
    "        cols = ['blue', 'orangered', 'green']\n",
    "        \n",
    "        fig = plt.figure(0)\n",
    "        fig.suptitle('PREDICTION METRICS', fontsize=16, fontname=\"Times New Roman\", fontweight='bold')\n",
    "        fig.set_figheight(10)\n",
    "        fig.set_figwidth(20)\n",
    "        axs = fig.subplots(2,2)\n",
    "        \n",
    "        rels = [[list(metrics_df.ACC_cl), list(metrics_df.ACC_cd), list(metrics_df.ACC)],\n",
    "                [list(metrics_df.MAE_cl), list(metrics_df.MAE_cd), list(metrics_df.MAE)],\n",
    "                [list(metrics_df.MSE_cl), list(metrics_df.MSE_cd), list(metrics_df.MSE)],\n",
    "                [list(metrics_df.RMSE_cl), list(metrics_df.RMSE_cd), list(metrics_df.RMSE)]]\n",
    "        titles = ['Accuracy', 'Mean Average Error', 'Mean Squared Error', 'Root Mean Squared Error']\n",
    "        labelss = [['ACC_cl', 'ACC_cd', 'ACC'], ['MAE_cl', 'MAE_cd', 'MAE'],\n",
    "                  ['MSE_cl', 'MSE_cd', 'MSE'], ['RMSE_cl', 'RMSE_cd', 'RMSE']]\n",
    "        axindxs = [[0, 0, 1, 1], [0, 1, 0, 1]]\n",
    "        \n",
    "        for rel, title, labels, i, j in zip(rels, titles, labelss, axindxs[0], axindxs[1]):\n",
    "            axs[i,j].set_title(title, fontsize=15, fontname=\"Times New Roman\", fontweight='bold')\n",
    "            axs[i,j].set_ylabel(title, fontsize=12, fontname=\"Times New Roman\")\n",
    "            x = np.arange(len(names))\n",
    "            w = 0.2\n",
    "            m = 0\n",
    "            for lst, col, label in zip(rel, cols, labels):\n",
    "                offset = w * m\n",
    "                bars = axs[i,j].bar(x+offset, lst, color=col, width=w/1.5, label=label)\n",
    "                axs[i,j].bar_label(bars, padding=0, fontsize=11, fontname=\"Times New Roman\")\n",
    "                m += 1\n",
    "            axs[i,j].set_ylim(0,max(rel[0]+rel[1]+rel[2])*1.25)\n",
    "            axs[i,j].legend()\n",
    "            axs[i,j].set_xticks(x+w)\n",
    "            axs[i,j].set_xticklabels(names, rotation=0) \n",
    "        \n",
    "    return metrics_df\n",
    "    \n",
    "\n",
    "def train_metrics(model, models, mets=['loss', 'ACC', 'MAE'], df_from='current', prnt=False, plot=False):\n",
    "    if df_from == 'current':\n",
    "        models = {model.name: model}\n",
    "        fitHistory = model.fitHistory\n",
    "\n",
    "    elif df_from == 'models':\n",
    "        models = models\n",
    "        fitHistory = [model.fitHistory for model in list(models.values())]\n",
    "    \n",
    "    if prnt:\n",
    "        print('=== TRAIN, VAL & EVAL METRICS ===')\n",
    "        for name, model in models.items():\n",
    "            print(f'MODEL : {name}')\n",
    "            train_loss = list(model.fitHistory.history.get(f'{mets[0]}'))[-1]\n",
    "            val_loss = list(model.fitHistory.history.get(f'val_{mets[0]}'))[-1]\n",
    "            print(f'{mets[0].upper()} : Training: {round(train_loss, 6)}, Validation: {round(val_loss, 6)}.')\n",
    "            train_acc = list(model.fitHistory.history.get(f'{mets[1]}'))[-1]\n",
    "            val_acc = list(model.fitHistory.history.get(f'val_{mets[1]}'))[-1]\n",
    "            print(f'{mets[1].upper()} : Training: {round(train_acc, 4)}, Validation: {round(val_acc, 4)}.')\n",
    "            if len(mets) == 3:\n",
    "                train_mae = list(model.fitHistory.history.get(f'{mets[2]}'))[-1]\n",
    "                val_mae = list(model.fitHistory.history.get(f'val_{mets[2]}'))[-1]\n",
    "                print(f'{mets[2].upper()} : Training: {round(train_mae, 4)}, Validation: {round(val_mae, 4)}.')\n",
    "            print(f'EVALUATE \\n Train: {model.trainEv}, \\n Test: {model.testEv}.', '\\n')\n",
    "    \n",
    "    if plot:\n",
    "        fig = plt.figure(1)\n",
    "        fig.suptitle('TRAINING & VALIDATION METRICS', fontsize=20, fontname=\"Times New Roman\", fontweight='bold')\n",
    "        fig.set_figheight(6)\n",
    "        fig.set_figwidth(20)\n",
    "        if len(mets) == 2:\n",
    "            ax1, ax2 = fig.subplots(1,2)\n",
    "        elif len(mets) == 3:\n",
    "            ax1, ax2, ax3 = fig.subplots(1,3)\n",
    "        fig.tight_layout(pad=2, h_pad=2.5, w_pad=5)\n",
    "        \n",
    "        ax1.set_title('Loss (MSE) v. Epochs', fontsize=20, fontname=\"Times New Roman\", fontweight='bold')\n",
    "        ax1.set_ylabel('MSE', fontsize=18, fontname=\"Times New Roman\")\n",
    "        ax1.set_xlabel('Epoch', fontsize=18, fontname=\"Times New Roman\")\n",
    "        ax1.set_yscale('log')\n",
    "        for name, model in models.items():\n",
    "            ax1.plot(model.fitHistory.history.get(f'{mets[0]}'), label=f'Training {mets[0].upper()} {name}')\n",
    "            ax1.plot(model.fitHistory.history.get(f'val_{mets[0]}'), label=f'Validation {mets[0].upper()} {name}')                    \n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.set_title('Accuracy v. Epochs', fontsize=20, fontname=\"Times New Roman\", fontweight='bold')\n",
    "        ax2.set_ylabel('Accuracy', fontsize=18, fontname=\"Times New Roman\")\n",
    "        ax2.set_xlabel('Epoch', fontsize=18, fontname=\"Times New Roman\")\n",
    "        for name, model in models.items():\n",
    "            ax2.plot(model.fitHistory.history.get(f'{mets[1]}'), label=f'Training {mets[1].upper()} {name}')\n",
    "            ax2.plot(model.fitHistory.history.get(f'val_{mets[1]}'), label=f'Validation {mets[1].upper()} {name}')\n",
    "        ax2.legend()\n",
    "        \n",
    "        if len(mets) == 3:\n",
    "            ax3.set_title('MAE v. Epochs', fontsize=20, fontname=\"Times New Roman\", fontweight='bold')\n",
    "            ax3.set_ylabel('MAE', fontsize=18, fontname=\"Times New Roman\")\n",
    "            ax3.set_xlabel('Epoch', fontsize=18, fontname=\"Times New Roman\")\n",
    "            ax3.set_yscale('log')\n",
    "            for name, model in models.items():\n",
    "                ax3.plot(model.fitHistory.history.get(f'{mets[2]}'), label=f'Training {mets[2].upper()} {name}')\n",
    "                ax3.plot(model.fitHistory.history.get(f'val_{mets[2]}'), label=f'Validation {mets[2].upper()} {name}')\n",
    "            ax3.legend()\n",
    "        \n",
    "    return fitHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651897c-3832-4f3c-82df-f25f9cb24bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_metrics_df = pred_metrics(Pmetrics_df, \n",
    "                               models, \n",
    "                               file='results/metrics/prediction-metrics.csv', \n",
    "                               df_from='models', \n",
    "                               add=None, \n",
    "                               df_save=False, \n",
    "                               prnt=True, \n",
    "                               plot=True)\n",
    "\n",
    "fitHistory = train_metrics(model=model,\n",
    "                           models=models, \n",
    "                           mets=['loss', 'ACC'], \n",
    "                           df_from='models', \n",
    "                           prnt=True, \n",
    "                           plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1562a-c674-4371-80a0-047589353bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predictions(output=None, name=None, re=None, file='results/predictions.csv', df_from='current', aerofoils_df=aerofoils_df,\n",
    "                model_add=False, df_save=False, plot=True, err=False):\n",
    "    if df_from == 'current':\n",
    "        df = output\n",
    "            \n",
    "    elif df_from == 'file':\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        \n",
    "    if model_add:\n",
    "        new_row = output\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "    \n",
    "    if df_save:\n",
    "        df.to_csv(file)\n",
    "    \n",
    "    NAMEs = list(set(df.file.tolist()))\n",
    "    REs = list(set(df.Re.tolist()))\n",
    "    print([re for re in REs])\n",
    "    print([name for name in NAMEs], '\\n')\n",
    "\n",
    "    plot_df = df[df.Re == re]\n",
    "    plot_df = plot_df[plot_df.file == name]\n",
    "    \n",
    "    if plot:\n",
    "        fig1 = plt.figure(2)\n",
    "        fig1.set_figheight(10)\n",
    "        fig1.set_figwidth(20)\n",
    "        fig1.suptitle(plot_df.name.tolist()[0].upper()+' || Re = {:,}'.format(int(plot_df.Re.tolist()[0])),\n",
    "                      fontsize=22, fontname=\"Times New Roman\", fontweight='bold')\n",
    "        axs = fig1.subplots(2,2)\n",
    "        fig1.tight_layout(pad=2, h_pad=5, w_pad=8)\n",
    "\n",
    "        axs[0,0].set_title('Lift Coefficient v. Angle of Attack', fontsize=20, fontname=\"Times New Roman\", fontweight='bold')\n",
    "        axs[0,0].set_xlabel('Angle of Attack [deg]', fontsize=18, fontname=\"Times New Roman\")\n",
    "        axs[0,0].set_ylabel('Lift Coefficient', fontsize=18, fontname=\"Times New Roman\")\n",
    "        a, t, p = plot_df.alpha.tolist(), plot_df.Cl.tolist(), plot_df.Cl_pred.tolist()\n",
    "        axs[0,0].plot(a, p, '--', lw=1, marker='o', markersize=2, label='Predicted')\n",
    "        axs[0,0].plot(a, t, lw=1, marker='o', markersize=2, label='True')\n",
    "        axs[0,0].legend()#bbox_to_anchor=(0.15,1))\n",
    "        \n",
    "        if err:\n",
    "            axs00 = axs[0,0].twinx()\n",
    "            axs00.set_ylabel('Error', fontsize=18, fontname=\"Times New Roman\", rotation=270, labelpad=15)\n",
    "            err = [np.abs(tt - pp) for tt, pp in zip(t, p)]\n",
    "            axs00.bar(a, err, width=(max(a)-min(a))/(2*len(a)), alpha=0.1, label='Error')\n",
    "            axs00.set_ylim(0, 1.5*max(err))\n",
    "            axs00.legend(bbox_to_anchor=(0.25,1))\n",
    "\n",
    "        axs[0,1].set_title('Lift Coefficient v. Angle of Attack', fontsize=20, fontname=\"Times New Roman\", fontweight='bold')\n",
    "        axs[0,1].set_xlabel('Angle of Attack [deg]', fontsize=18, fontname=\"Times New Roman\")\n",
    "        axs[0,1].set_ylabel('Drag Coefficient', fontsize=18, fontname=\"Times New Roman\")\n",
    "        a, t, p = plot_df.alpha.tolist(), plot_df.Cd.tolist(), plot_df.Cd_pred.tolist()\n",
    "        axs[0,1].plot(a, p, '--', lw=1, marker='o', markersize=2, label='Predicted')\n",
    "        axs[0,1].plot(a, t, lw=1, marker='o', markersize=2, label='True')\n",
    "        axs[0,1].legend()#bbox_to_anchor=(0.3,1))\n",
    "        \n",
    "        if err:\n",
    "            axs01 = axs[0,1].twinx()\n",
    "            axs01.set_ylabel('Error', fontsize=18, fontname=\"Times New Roman\", rotation=270, labelpad=15)\n",
    "            err = [np.abs(tt - pp) for tt, pp in zip(t, p)]\n",
    "            axs01.bar(a, err, width=(max(a)-min(a))/(2*len(a)), alpha=0.1, label='Error')\n",
    "            axs01.set_ylim(0, 1.5*max(err))\n",
    "            axs01.legend(bbox_to_anchor=(0.4,1))\n",
    "        \n",
    "        axs[1,0].set_title('Lift to Drag Ratio v. Angle of Attack', fontsize=20, fontname=\"Times New Roman\", fontweight='bold')\n",
    "        axs[1,0].set_xlabel('Angle of Attack [deg]', fontsize=18, fontname=\"Times New Roman\")\n",
    "        axs[1,0].set_ylabel('Lift to Drag Ratio', fontsize=18, fontname=\"Times New Roman\")\n",
    "        a, t, p = plot_df.alpha.tolist(), plot_df.LtD.tolist(), plot_df.LtD_pred.tolist()\n",
    "        axs[1,0].plot(a, p, '--', lw=1, marker='o', markersize=2, label='Predicted')\n",
    "        axs[1,0].plot(a, t, lw=1, marker='o', markersize=2, label='True')\n",
    "        axs[1,0].legend()#bbox_to_anchor=(0.15,1))\n",
    "        \n",
    "        if err:\n",
    "            axs10 = axs[1,0].twinx()\n",
    "            axs10.set_ylabel('Error', fontsize=18, fontname=\"Times New Roman\", rotation=270, labelpad=15)\n",
    "            err = [np.abs(tt - pp) for tt, pp in zip(t, p)]\n",
    "            axs10.bar(a, err, width=(max(a)-min(a))/(2*len(a)), alpha=0.1, label='Error')\n",
    "            axs10.set_ylim(0, 1.5*max(err))\n",
    "            axs10.legend(bbox_to_anchor=(0.25,1))\n",
    "        \n",
    "        aindx = aerofoils_df.loc[aerofoils_df.file == name].index[0]\n",
    "        plot_profile(aerofoils_df, aindx, scatt=False, x_val=False, ax=axs[1,1], prnt=False)\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675649d-8e79-4416-ba66-e1ec02c96373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_df = predictions(output=output_df, \n",
    "                      name='n0012', \n",
    "                      re=1000000, \n",
    "                      file='results/predictions.csv', \n",
    "                      df_from='current', \n",
    "                      aerofoils_df=aerofoils_df, \n",
    "                      model_add=False, \n",
    "                      df_save=False, \n",
    "                      plot=True, \n",
    "                      err=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b565d702-aeb7-4214-91db-495562030e08",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
